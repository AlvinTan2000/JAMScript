Asssumptions
============

    Cloud has many J-cores. All J-cores are synchronized using a Paxos or RAFT like consensus
    protocol so that the updates are propagated to all J-cores at that level.

    For now, a Fog has only one J-core. However, we need to relax this assumption so that a Fog
    can include many J-cores. In that case, we need to have a consensus algorithm across the J-cores
    like the Cloud.

Assumptions on Fault Characteristics
====================================

    We assume the Fog interconnections to be relatively stable. That is, the disconnection times within the
    Fog to be very small. The Fog itself should be available most of the time. The devices should remain
    attached to the Fog most of the time. May be due to hibernation or power saving concerns the devices
    might go offline.


J-Core Implementation Plan
==========================

J-Core with PUB and REP capability
    = PUB capability: open the socket, hookup the handler for publishing the events.
    = REP capability: open the socket, hookup the handler for receiving the connection and sending the reply.

J-Core should implement the following scenarios:
    Launch test beacons to ask C-cores to send test function calls
    Measure the following:
        (a) Group size
        (b) Average times to respond.. median * N could be the timeout?
        (c) J-core could be started with the multiplier that should be used for the timeout value

C-core launches a function execution:
    A function execution call request from C-core
        = We could use the same function call to implement synchronous and asynchronous calls.
        = Request comes in for function execution. We run the function and return the results.

J-core launches a function for execution:
    Synchronous execution versus asynchronous execution

    Publish the request for execution through the PUB socket to all C-cores
    C-cores send the consent to execute to the J-core as a request. The J-core sends
    the reply back and notes the execution of the C-core. Now the C-core needs to complete the
    execution and get back.

    There could be two timeouts: First timeout if no C-cores respond within the average time to respond.
    This is a function of the measured values from the benchmarking activities and the configuration
    parameters offered at startup.

    Second timeout is more complex. We want this to figure out whether a C-core is wanting more time versus the
    C-core just walking away from the computing activity. We suggest the C-core to ask for a lease -- how long
    it wants for executing the workload. It could use prior activity times to guess it correct. Otherwise, it
    would arbitrarily ask a lease. If it completes the execution within the lease, all is well. Otherwise,
    the C-core is responsible for extending the lease. Otherwise, the J-core will timeout the node and
    figure that the C-core has left or unresponsive.

    This allows to some C-cores to ask for more time. There could be a maximum time duration for a lease duration
    and also number of lease extensions. So the C-core may not be able to just keep on asking for extensions.

IMPORTANT: J-core ... Fog versus Cloud level. We could launch activities from J-Cores at the Cloud or Fog levels.
If we launch at the Cloud level, we have global scope for the execution. Again the cloud should use a timeout value
to limit the computation explosion. When the J-cores are started the timeout multipliers should be given. These multipliers
are used with the measured values we see from the benchmarking phase.

Now lets launch the execution from a Fog J-core. What happens? The Fog J-core also has a multiplier specified at the startup
which can be used to set the timeout values. The cloud is responsible for setting the timeout multiplers for the Fogs so
that a Fog can propagate its queries to other Fogs. This could be done many parameters that the cloud could be taking into
consideration such as number of nodes connected, proximity, number of interesting services or devices in a particular Fog.
For now, we will assume that this value is set. We can determine an algorithm for setting this parameter very soon.

IMPORTANT:

NOTE 1: Reliability issue. What do we have to do when the request comes in from C-core for execution?
How can we do reliability management? Log every incoming request for execution as incoming request and
every outgoing result.

NOTE 2: JAM machine should to introspective benchmarking very often so that it can set the
timeout values in the proper way.

Pseudo Code and Design for the J-Core
=====================================

Initialize Routine::

Open the REP_ENDPOINT. If successfull, hookup the REP handler at the J-Core.
    The REP handler should do the following:
        When a REP message comes in, we examine what the message is and then react accordingly
    NOTE: REP_ENDPOINT is never locally triggerable. It is always reacting for remote excitations.

Open the PUB_ENDPOINT. If successfull, hookup the PUB handler at the J-Core.
    The PUB handler should do the following:
        It should watch the pubQueue and push those messages onto the network through the END_POINT.

Fog Ping Routine::

    Write the Ping Message to the pubQueue.
        // The Ping Message should have a FogID so only members of the particular Fog respond to the ping request.
        // We have already setup the intra Fog PUB-SUB like the above so that when we send it to the group only
        // those members are getting it.
    The Ping Message should have the reply endpoint that should be used to respond to the ping query.
    So we broadcast the ping query and expect the respondents to reply individually.

    The Ping Routine waits for some time (predefined time) and checks the Ping Replies List. This will include the list of
    Fog members. Their reply times (delay values), etc.

    The Fog Ping routine returns these values as its result.

    We need a Ping REP handler that takes care of the REP processing. The REP handler is guaranteed to be hooked up
    where we advertised it to be.. unless the service (Fog) has managed to go down!

    When a REP comes in as a Ping Reply, it is going to examine the Ping Replies List corresponding to the Ping-ID
    value. If the Ping Reply is still outstanding, it is going to insert the reply into the Ping Replies List and get the
    rank of the replier in that list and then go back to the replying C-core. If the Ping Reply List is closed, we
    also let the C-core know that the Ping Reply is closed because the node was too slow in responding!
    May be the slow C-cores might want to register their late replies in the Ping Replies List so that the timeout setting
    routine could set the timeout accordingly in the next iteration.

Fog Inter-Ping Routine::

This may not be done properly. I don't like the fact that subscribers need to know who their senders are.. at
least in ZeroMQ. May be MQTT might actually help here.

Each Fog (one server for now) has a dedicated channel for publishing inter-fog pings. Each fog is still
listening on each Fog known to it. This itself could be known by the cloud. That is the cloud
has informed the fogs about the endpoints on which they could subscribe for inter-fog communications.

Each Fog is subscribing to the inter-Fog endpoint.
A Fog wants to ping other Fogs that are around or were aware of it, it just sends the ping request to the
pubQueue on the "inter-Fog" channel. If it does not get any response or very few responses,
it might initiate an advertisement through the cloud.

We use the same procedure as above. The advertisement includes the end point for reply processing.
After receiving the Inter-ping replies we get to know how many Fogs are available as neighbors.

Cloud Ping Routine::

Like the Fog, the Cloud is also publishing its broadcast request message. The request is published onto
its pubQueue. The REQ_ENDPOINT at which the replies should arrive is also advertised as part of the
ping.

Each incoming Ping Reply is processed as it is done with the Fog. We could reuse the same functions
here. The only thing that is different is the server addresses.

This is ping is supposed to target just the Fogs. No devices are included here whereas the
Fog could be pinging for devices as well as other Fogs.


Cloud Level or Global Advertisements Routine::

This is sent out from the same PUB_ENDPOINT but in a different channel. This channel should allow us
to differentiate between the normal pings and the advertisements. Also, the advertisements do not have
a back channel. There is no confirmation to the publisher on the number of receipients or the fact
the publications were consumed by any Fogs.

Run a J-core Activity::

The device is launching the request to run the activity. We have synchronous and asynchronous
activities. The major difference is that the device would be waiting for the completion of the
synchronous activity. Also, the synchronous activity just results in a return value, whereas
the asynchronous activity can lead to a callback.

The device node (C-core) sends a call to the REP_ENDPOINT. Asking for the execution of a particular activity.
The J-core reponds with a positive reply and a lease time for the duration of the activity. If the
C-core agrees to the lease duration it enters the value in its table and waits. If not, it asks for
an adjustment of the value because it is too large and does not satisfy its requirements. In either
case, the C-core sends a confirmation/acknowledgement back to the J-core. The J-core does not wait
for that acknowledgement to start the execution of the activity.

We could have a "single thread of execution" problem here.
To overcome this problem we could yield the execution using "setImmediate()" is the Node.js program.
Need to figure out how things can be done inside the web browser.

How could a machine know that it did not fail?
Self failure detection... is this an interesting problem??

We could maintain a soft clock and assume a hard real-time clock maintained with a independent battery
backup. If the soft clock is behind the hard clock by more than T seconds, we can say there was a reboot.

Device handovers the job of running the activity to the J-core. It gets the confirmation that the J-core
is going to run it. When the activity starts running there should be event published on the pubQueue..
so the device can start its lease clock.

Once the lease clock expires the device would contact the J-core to retreive the results of the
activity execution. It expects to get a correct results or error. It could also get a lease extension
notification. In that case, the device has somehow missed the lease extension that was published
on the pubQueue.

The device would also contact the J-core if it receives a lease extension list that does not include
its lease. It makes sense to have multiple activities on lease because the J-core could be interleaving
activities due to I/O. So a JS portion of the activity asks for an I/O action and the JS portion gets
enqueued.


Run a C-core Activity::

C-core activity is launched by the J-cores on the devices. The J-cores in touch with the devices
are the Fogs so they are the ones launching C-core activity.

There could be an exception. We could have C-cores directly connected to the cloud J-cores as well.

J-cores send a Request for Activity (RFA) on the pubQueue. The C-cores bid on the activity.
The RFA could have conditions that specify which C-cores can participate in a particular activity.

C-cores offer to run the activity and J-core accepts the offer. The J-core would ask for periodic
progress reports and it is does not receive them, it could timeout those activities.

The C-core is responsible for depositing the final result: success or error. In either case
a return value could be transfered to the J-core. The J-core uses these values to trigger appropriate
callback.


Idea for Implementing Security
==============================

Each program will have a program key that is shared among all the components of the program.
When the program is started it is given a machine key. The machine key and program key should be
same for the program components to talk with each other. A program can be started with multiple
machine keys. Obviously, different machine keys would correspond to different machine names.
The etcd based JAMScript registry will hold the machine to key bindings.

We could look into the self-certifying file system idea and create self-certifying messages
are verifiable in a distributed manner. If a message is verifiable, it is thrown away. No processing
happens for such messages.


Remote Procedure Call Protocol
==============================

We could have an interesting RPC protocol, which is based on PUB-SUB followed by a REQ-REP.
Is this better than a protocol based solely on PUB-SUB??
What are the protocol building blocks we could use? What is the reliability we could offer?
Client failure, service (server) failure, both failure, server failover? how do we maintain
service state? RPC can be seen as a high-level contract than REST or other state-less protocols.
