Asssumptions
============

    Cloud has many J-cores. All J-cores are synchronized using a Paxos or RAFT like consensus
    protocol so that the updates are propagated to all J-cores at that level.

    For now, a Fog has only one J-core. However, we need to relax this assumption so that a Fog
    can include many J-cores. In that case, we need to have a consensus algorithm across the J-cores
    like the Cloud.

J-Core Implementation Plan
==========================

J-Core with PUB and REP capability
    = PUB capability: open the socket, hookup the handler for publishing the events.
    = REP capability: open the socket, hookup the handler for receiving the connection and sending the reply.

J-Core should implement the following scenarios:
    Launch test beacons to ask C-cores to send test function calls
    Measure the following:
        (a) Group size
        (b) Average times to respond.. median * N could be the timeout?
        (c) J-core could be started with the multiplier that should be used for the timeout value

C-core launches a function execution:
    A function execution call request from C-core
        = We could use the same function call to implement synchronous and asynchronous calls.
        = Request comes in for function execution. We run the function and return the results.

J-core launches a function for execution:
    Synchronous execution versus asynchronous execution

    Publish the request for execution through the PUB socket to all C-cores
    C-cores send the consent to execute to the J-core as a request. The J-core sends
    the reply back and notes the execution of the C-core. Now the C-core needs to complete the
    execution and get back.

    There could be two timeouts: First timeout if no C-cores respond within the average time to respond.
    This is a function of the measured values from the benchmarking activities and the configuration
    parameters offered at startup.

    Second timeout is more complex. We want this to figure out whether a C-core is wanting more time versus the
    C-core just walking away from the computing activity. We suggest the C-core to ask for a lease -- how long
    it wants for executing the workload. It could use prior activity times to guess it correct. Otherwise, it
    would arbitrarily ask a lease. If it completes the execution within the lease, all is well. Otherwise,
    the C-core is responsible for extending the lease. Otherwise, the J-core will timeout the node and
    figure that the C-core has left or unresponsive.

    This allows to some C-cores to ask for more time. There could be a maximum time duration for a lease duration
    and also number of lease extensions. So the C-core may not be able to just keep on asking for extensions.

IMPORTANT: J-core ... Fog versus Cloud level. We could launch activities from J-Cores at the Cloud or Fog levels.
If we launch at the Cloud level, we have global scope for the execution. Again the cloud should use a timeout value
to limit the computation explosion. When the J-cores are started the timeout multipliers should be given. These multipliers
are used with the measured values we see from the benchmarking phase.

Now lets launch the execution from a Fog J-core. What happens? The Fog J-core also has a multiplier specified at the startup
which can be used to set the timeout values. The cloud is responsible for setting the timeout multiplers for the Fogs so
that a Fog can propagate its queries to other Fogs. This could be done many parameters that the cloud could be taking into
consideration such as number of nodes connected, proximity, number of interesting services or devices in a particular Fog.
For now, we will assume that this value is set. We can determine an algorithm for setting this parameter very soon.


IMPORTANT:

NOTE 1: Reliability issue. What do we have to do when the request comes in from C-core for execution?
How can we do reliability management? Log every incoming request for execution as incoming request and
every outgoing result.

NOTE 2: JAM machine should to introspective benchmarking very often so that it can set the
timeout values in the proper way.


Pseudo Code and Design for the J-Core
=====================================

Initialize Routine::

Open the REP_ENDPOINT. If successfull, hookup the REP handler at the J-Core.
    The REP handler should do the following:
        When a REP message comes in, we examine what the message is and then react accordingly
    NOTE: REP_ENDPOINT is never locally triggerable. It is always reacting for remote excitations.

Open the PUB_ENDPOINT. If successfull, hookup the PUB handler at the J-Core.
    The PUB handler should do the following:
        It should watch the pubQueue and push those messages onto the network through the END_POINT.

Fog Ping Routine::

    Write the Ping Message to the pubQueue.
        // The Ping Message should have a FogID so only members of the particular Fog respond to the ping request.
        // We have already setup the intra Fog PUB-SUB like the above so that when we send it to the group only
        // those members are getting it.
    The Ping Message should have the reply endpoint that should be used to respond to the ping query.
    So we broadcast the ping query and expect the respondents to reply individually.

    The Ping Routine waits for some time (predefined time) and checks the Ping Replies List. This will include the list of
    Fog members. Their reply times (delay values), etc.

    The Fog Ping routine returns these values as its result.

    We need a Ping REP handler that takes care of the REP processing. The REP handler is guaranteed to be hooked up
    where we advertised it to be.. unless the service (Fog) has managed to go down!

    When a REP comes in as a Ping Reply, it is going to examine the Ping Replies List corresponding to the Ping-ID
    value. If the Ping Reply is still outstanding, it is going to insert the reply into the Ping Replies List and get the
    rank of the replier in that list and then go back to the replying C-core. If the Ping Reply List is closed, we
    also let the C-core know that the Ping Reply is closed because the node was too slow in responding!
    May be the slow C-cores might want to register their late replies in the Ping Replies List so that the timeout setting
    routine could set the timeout accordingly in the next iteration.

Fog Inter-Ping Routine::

This may not be done properly. I don't like the fact that subscribers need to know who their senders are.. at
least in ZeroMQ. May be MQTT might actually help here.

Each Fog (one server for now) has a dedicated channel for publishing inter-fog pings. Each fog is still
listening on each Fog known to it. This itself could be known by the cloud. That is the cloud
has informed the fogs about the endpoints on which they could subscribe for inter-fog communications.

Each Fog is subscribing to the inter-Fog endpoint.
A Fog wants to ping other Fogs that are around or were aware of it, it just sends the ping request to the
pubQueue on the "inter-Fog" channel. If it does not get any response or very few responses,
it might initiate an advertisement through the cloud.

We use the same procedure as above. The advertisement includes the end point for reply processing.
After receiving the Inter-ping replies we get to know how many Fogs are available as neighbors.

Cloud Ping Routine::

Reliability at the Cloud level.




Run a J-core Activity::


Run a C-core Activity::







Idea for Implementing Security
==============================

Each program will have a program key that is shared among all the components of the program.
When the program is started it is given a machine key. The machine key and program key should be
same for the program components to talk with each other. A program can be started with multiple
machine keys. Obviously, different machine keys would correspond to different machine names.
The etcd based JAMScript registry will hold the machine to key bindings.

We could look into the self-certifying file system idea and create self-certifying messages
are verifiable in a distributed manner. If a message is verifiable, it is thrown away. No processing
happens for such messages.


Remote Procedure Call Protocol
==============================

We could have an interesting RPC protocol, which is based on PUB-SUB followed by a REQ-REP.
Is this better than a protocol based solely on PUB-SUB??
What are the protocol building blocks we could use? What is the reliability we could offer?
Client failure, service (server) failure, both failure, server failover? how do we maintain
service state? RPC can be seen as a high-level contract than REST or other state-less protocols.
